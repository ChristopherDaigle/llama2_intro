{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438b5bcd-5de9-471f-b8c9-b65a7e39d943",
   "metadata": {},
   "source": [
    "# Working with a a Foundation Model (FM) on a Mac\n",
    "\n",
    "1. Establish environment\n",
    "2. Retrieve model\n",
    "3. Alter model (quantize, finetune, merge, etc.)\n",
    "4. Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18911ee8-6d44-436e-9100-d4d75842e72e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Establish Environment\n",
    "\n",
    "To begin, it is important the environment on your system is set. As I am using a MacBookPro with an M1, there will be some idiosyncracies that have to be accounted for. Namely, Apple does not integrate with Nvidia hardware, nicely or at all, to the best of my knowledge, even with an eGPU. So, it is required I do a few things before I can work with most foundation models (FMs) as they typically depend on CUDA, an Nvidia item. Instead, I will leverage llama.cpp from GGML.\n",
    "\n",
    "So, here are the steps:\n",
    "* Pre-requisites:\n",
    "> Mac computers with Apple silicon or AMD GPUs<br>\n",
    "> macOS 12.3 or later<br>\n",
    "> Python 3.7 or later<br>\n",
    "> Xcode command-line tools: `xcode-select --install`<br>\n",
    "\n",
    "[Note: instructions from Apple](https://developer.apple.com/metal/pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a270a3-37ef-425a-b785-b7d4219ca5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
